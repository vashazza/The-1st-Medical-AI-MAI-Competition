{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lci0_xCqK81",
        "outputId": "73cee18c-b973-4a69-ca94-708602c4ff1d"
      },
      "id": "9lci0_xCqK81",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import SwinForImageClassification, AutoFeatureExtractor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "rsU9GNnZMmXg"
      },
      "id": "rsU9GNnZMmXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'EPOCHS': 10,\n",
        "    'LEARNING_RATE': 5e-5,\n",
        "    'BATCH_SIZE': 32,\n",
        "    'SEED': 41\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "n0H_C3BNNMpR"
      },
      "id": "n0H_C3BNNMpR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels=None, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # 이미지를 RGB로 변환\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.float)  # 다중 레이블 벡터로 변환\n",
        "            return image, label\n",
        "        return image"
      ],
      "metadata": {
        "id": "XgM1SRXqNQ47"
      },
      "id": "XgM1SRXqNQ47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유전자 개수에 맞게 출력 노드 수정\n",
        "num_genes = 3467  # 유전자 수를 데이터에 맞게 설정\n",
        "\n",
        "model = SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n",
        "model.classifier = nn.Linear(model.classifier.in_features, num_genes)  # 유전자 개수에 맞게 출력 설정\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJTA1cZpNVpM",
        "outputId": "4ac153f7-c500-4bfb-b4e9-1fd3b5c6a8ff"
      },
      "id": "TJTA1cZpNVpM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SwinForImageClassification(\n",
              "  (swin): SwinModel(\n",
              "    (embeddings): SwinEmbeddings(\n",
              "      (patch_embeddings): SwinPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "      )\n",
              "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): SwinEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): SwinStage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x SwinLayer(\n",
              "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "              (attention): SwinAttention(\n",
              "                (self): SwinSelfAttention(\n",
              "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
              "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
              "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): SwinSelfOutput(\n",
              "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (drop_path): SwinDropPath(p=0.1)\n",
              "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "              (intermediate): SwinIntermediate(\n",
              "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): SwinOutput(\n",
              "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (downsample): SwinPatchMerging(\n",
              "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
              "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (1): SwinStage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x SwinLayer(\n",
              "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (attention): SwinAttention(\n",
              "                (self): SwinSelfAttention(\n",
              "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): SwinSelfOutput(\n",
              "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (drop_path): SwinDropPath(p=0.1)\n",
              "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (intermediate): SwinIntermediate(\n",
              "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): SwinOutput(\n",
              "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (downsample): SwinPatchMerging(\n",
              "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
              "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (2): SwinStage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-5): 6 x SwinLayer(\n",
              "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "              (attention): SwinAttention(\n",
              "                (self): SwinSelfAttention(\n",
              "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): SwinSelfOutput(\n",
              "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (drop_path): SwinDropPath(p=0.1)\n",
              "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "              (intermediate): SwinIntermediate(\n",
              "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): SwinOutput(\n",
              "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (downsample): SwinPatchMerging(\n",
              "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
              "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (3): SwinStage(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x SwinLayer(\n",
              "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (attention): SwinAttention(\n",
              "                (self): SwinSelfAttention(\n",
              "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): SwinSelfOutput(\n",
              "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (drop_path): SwinDropPath(p=0.1)\n",
              "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (intermediate): SwinIntermediate(\n",
              "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): SwinOutput(\n",
              "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=3467, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/2024_MAI_DACON/Open/'\n",
        "%cd /content/drive/MyDrive/2024_MAI_DACON/Open\n",
        "\n",
        "df = pd.read_csv(os.path.join(ROOT_DIR, 'train.csv'))\n",
        "train_len = int(len(df) * 0.8)\n",
        "train_df = df.iloc[:train_len]\n",
        "val_df = df.iloc[train_len:]\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "train_label_vec = train_df.iloc[:,2:].values.astype(np.float32)\n",
        "val_label_vec = val_df.iloc[:,2:].values.astype(np.float32)\n",
        "\n",
        "CFG['label_size'] = train_label_vec.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oDDpP8xqYOU",
        "outputId": "fc45b306-a0d5-48db-f162-f743a1d9477b"
      },
      "id": "5oDDpP8xqYOU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2024_MAI_DACON/Open\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 수평 뒤집기\n",
        "    transforms.RandomVerticalFlip(p=0.5),  # 수직 뒤집기\n",
        "    transforms.RandomRotation(degrees=15),  # 회전\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 밝기, 대비, 채도 조절\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # 임의 크기로 자르기\n",
        "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),  # 가우시안 블러\n",
        "    transforms.ToTensor(),  # 텐서 변환\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
        "])"
      ],
      "metadata": {
        "id": "k83YOA2lOCxs"
      },
      "id": "k83YOA2lOCxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extractor 불러오기\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n",
        "\n",
        "# 데이터셋 정의\n",
        "train_dataset = CustomDataset(train_df['path'].values, train_label_vec,  transform=transform)\n",
        "val_dataset = CustomDataset(val_df['path'], val_label_vec, transform=transform)\n",
        "\n",
        "# 데이터 로더 정의\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVlQOc14NcNs",
        "outputId": "ce49543d-7e18-479c-8eaf-dc75a40eed88"
      },
      "id": "zVlQOc14NcNs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def validation_with_metrics(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # tqdm으로 검증 진행 상황 표시\n",
        "    val_progress = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_progress:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            outputs = outputs.logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # 현재 배치의 손실 표시\n",
        "            val_progress.set_postfix({\"Val Loss\": loss.item()})\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    return avg_val_loss  # 상관계수 계산 없이 검증 손실만 반환\n",
        "\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "def train_model_with_metrics(model, train_loader, val_loader, optimizer, criterion, device):\n",
        "    best_val_loss = float('inf')  # 초기 값은 무한대로 설정\n",
        "\n",
        "    for epoch in range(CFG['EPOCHS']):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # tqdm으로 학습 진행 상황 표시\n",
        "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG['EPOCHS']} [Training]\", leave=False)\n",
        "        for imgs, labels in train_progress:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            outputs = outputs.logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # 현재 배치의 손실 표시\n",
        "            train_progress.set_postfix({\"Train Loss\": loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation Loss 계산\n",
        "        avg_val_loss = validation_with_metrics(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch+1}/{CFG['EPOCHS']}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Validation Loss가 가장 낮은 모델을 저장\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)  # 모델의 가중치 저장\n",
        "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# 학습 실행\n",
        "train_model_with_metrics(model, train_loader, val_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck_3IrchQ2LM",
        "outputId": "debe14f2-92f5-42d1-b850-221ee73c2500"
      },
      "id": "Ck_3IrchQ2LM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/10], Train Loss: 0.0461, Val Loss: 0.0459\n",
            "Best model saved at epoch 1 with validation loss: 0.0459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [2/10], Train Loss: 0.0459, Val Loss: 0.0457\n",
            "Best model saved at epoch 2 with validation loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [3/10], Train Loss: 0.0459, Val Loss: 0.0457\n",
            "Best model saved at epoch 3 with validation loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [4/10], Train Loss: 0.0458, Val Loss: 0.0457\n",
            "Best model saved at epoch 4 with validation loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [5/10], Train Loss: 0.0458, Val Loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [6/10], Train Loss: 0.0458, Val Loss: 0.0457\n",
            "Best model saved at epoch 6 with validation loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [7/10], Train Loss: 0.0457, Val Loss: 0.0457\n",
            "Best model saved at epoch 7 with validation loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [8/10], Train Loss: 0.0457, Val Loss: 0.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [9/10], Train Loss: 0.0457, Val Loss: 0.0456\n",
            "Best model saved at epoch 9 with validation loss: 0.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [10/10], Train Loss: 0.0456, Val Loss: 0.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Swin Transformer 입력 크기\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "IneaQvqTcB1a"
      },
      "id": "IneaQvqTcB1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, img_paths, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "vjVWAoSTcC3B"
      },
      "id": "vjVWAoSTcC3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('./test.csv')\n",
        "\n",
        "test_dataset = CustomDataset(test['path'].values, None, test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "mQZu6CqJcH3k"
      },
      "id": "mQZu6CqJcH3k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, test_loader, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():  # 예측 시 그라디언트 계산 비활성화\n",
        "        for imgs in test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            outputs = outputs.logits  # logits 사용\n",
        "            predictions.append(outputs.cpu().numpy())  # GPU에서 CPU로 변환 후 numpy로 변환\n",
        "\n",
        "    return np.concatenate(predictions)  # 리스트로 반환된 값을 numpy 배열로 변환"
      ],
      "metadata": {
        "id": "WN78xTmhcVyd"
      },
      "id": "WN78xTmhcVyd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 수행\n",
        "preds = inference(model, test_loader, device)\n",
        "\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit.iloc[:, 1:] = np.array(preds).astype(np.float32)\n",
        "submit.to_csv('./Swin_10_submit.csv', index=False)"
      ],
      "metadata": {
        "id": "de5BhOyzcboH"
      },
      "id": "de5BhOyzcboH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gimin_py38",
      "language": "python",
      "name": "gimin_py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}